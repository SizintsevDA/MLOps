{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# XGBoost is all you need, Maybe...\nHello Kaggle, in this Notebook I will try to push the limits of XGBoost, let see how I will do...\nI will take a lot of inspiration from this Book, basically as I read the book I will implement all the tips in this notebook...\n\n**Hands-On Gradient Boosting with XGBoost and scikit-learn: Perform accessible machine learning and extreme gradient boosting with Python**\n\nhttps://www.oreilly.com/library/view/hands-on-gradient-boosting/9781839218354/\n\n**Objective:**\nBuild a powerfull XGBoost Model that can provide a good estimation.\n\n**Strategy:**\nI think I will follow this strategy:\n* Contruct aggregated features, Min, Max. Mad, Var, Sum and Others. I will need to identify the best grouping strategy to create this features. -- Completed\n* Contruct lag features -- Working On\n* Contruct rolling features -- Working On\n* Contruct Expanding features -- Working On\n* Other ideas that I'm not sure at this point -- Researching","metadata":{}},{"cell_type":"markdown","source":"### Data Descriptions\nIn this competition, you'll classify 60-second sequences of sensor data, indicating whether a subject was in either of two activity states for the duration of the sequence\n\n### Files and Field Descriptions\ntrain.csv - the training set, comprising ~26,000 60-second recordings of thirteen biological sensors for almost one thousand experimental participants\n* sequence - a unique id for each sequence\n* subject - a unique id for the subject in the experiment\n* step - time step of the recording, in one second intervals\n* sensor_00 - sensor_12 - the value for each of the thirteen sensors at that time step\n* train_labels.csv - the class label for each sequence.\n* sequence - the unique id for each sequence.\n* state - the state associated to each sequence. This is the target which you are trying to predict.\n\ntest.csv - the test set. For each of the ~12,000 sequences, you should predict a value for that sequence's state.\n\nsample_submission.csv - a sample submission file in the correct format.","metadata":{}},{"cell_type":"markdown","source":"---","metadata":{"execution":{"iopub.status.busy":"2022-04-02T20:36:44.124663Z","iopub.execute_input":"2022-04-02T20:36:44.125343Z","iopub.status.idle":"2022-04-02T20:36:44.130218Z","shell.execute_reply.started":"2022-04-02T20:36:44.125308Z","shell.execute_reply":"2022-04-02T20:36:44.129232Z"}}},{"cell_type":"markdown","source":"# 1. Loading the Requiered Libraries\nImpoerting the typical set of libraries to create a ML model, I tried to keep this to the minimun possible.","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-04-03T02:19:35.950317Z","iopub.execute_input":"2022-04-03T02:19:35.950874Z","iopub.status.idle":"2022-04-03T02:19:35.96315Z","shell.execute_reply.started":"2022-04-03T02:19:35.950834Z","shell.execute_reply":"2022-04-03T02:19:35.962385Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import warnings","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"___","metadata":{}},{"cell_type":"markdown","source":"# 2. Setting the Notebook\nIn this section I will configure some of the default parameters for my notebook execution. for example number of decimals, warning and numbers of rows I will like to load in case the dataset is to massive.","metadata":{}},{"cell_type":"code","source":"%%time\n# I like to disable my Notebook Warnings.\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"execution":{"iopub.status.busy":"2022-04-03T02:19:35.979412Z","iopub.execute_input":"2022-04-03T02:19:35.979695Z","iopub.status.idle":"2022-04-03T02:19:35.984923Z","shell.execute_reply.started":"2022-04-03T02:19:35.979667Z","shell.execute_reply":"2022-04-03T02:19:35.984127Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Notebook Configuration...\n\n# Amount of data we want to load into the Model...\nDATA_ROWS = None\n# Dataframe, the amount of rows and cols to visualize...\nNROWS = 100\nNCOLS = 15\n# Main data location path...\nBASE_PATH = '...'","metadata":{"execution":{"iopub.status.busy":"2022-04-03T02:19:36.06704Z","iopub.execute_input":"2022-04-03T02:19:36.067351Z","iopub.status.idle":"2022-04-03T02:19:36.072516Z","shell.execute_reply.started":"2022-04-03T02:19:36.067318Z","shell.execute_reply":"2022-04-03T02:19:36.071757Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Configure notebook display settings to only use 2 decimal places, tables look nicer.\npd.options.display.float_format = '{:,.2f}'.format\npd.set_option('display.max_columns', NCOLS) \npd.set_option('display.max_rows', NROWS)","metadata":{"execution":{"iopub.status.busy":"2022-04-03T02:19:36.13869Z","iopub.execute_input":"2022-04-03T02:19:36.138978Z","iopub.status.idle":"2022-04-03T02:19:36.144389Z","shell.execute_reply.started":"2022-04-03T02:19:36.13895Z","shell.execute_reply":"2022-04-03T02:19:36.143594Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"___","metadata":{}},{"cell_type":"markdown","source":"# 3. Loading the Information (CSV) Into A Dataframe\nIn this section I just import the datasets CSVs using Pandas, not much to see here this is a simple step","metadata":{}},{"cell_type":"code","source":"%%time\n# Load the CSVs into a pandas dataframe for future data manipulation.\ntrn_data = pd.read_csv('/kaggle/input/tabular-playground-series-apr-2022/train.csv')\ntrn_label_data = pd.read_csv('/kaggle/input/tabular-playground-series-apr-2022/train_labels.csv')\ntst_data = pd.read_csv('/kaggle/input/tabular-playground-series-apr-2022/test.csv')\n\nsub = pd.read_csv('/kaggle/input/tabular-playground-series-apr-2022/sample_submission.csv')","metadata":{"execution":{"iopub.status.busy":"2022-04-03T02:28:32.713941Z","iopub.execute_input":"2022-04-03T02:28:32.714693Z","iopub.status.idle":"2022-04-03T02:28:38.994124Z","shell.execute_reply.started":"2022-04-03T02:28:32.71465Z","shell.execute_reply":"2022-04-03T02:28:38.993372Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4. Exploring the Information Available\nThe typical quick exploration to get an idea of the datsets loaded. I ussually like to load the information of the dataset, number of variables, type of variables.\nas also visualizing the first few rowns in the dataframe.\n","metadata":{}},{"cell_type":"markdown","source":"## 4.1. Analysing the Trian Dataset\nSimple analysis of the train dataset I ussually run\ninfo, head, descrive and number of nulls","metadata":{}},{"cell_type":"code","source":"%%time\ntrn_data.info()","metadata":{"execution":{"iopub.status.busy":"2022-04-03T02:19:42.075983Z","iopub.execute_input":"2022-04-03T02:19:42.076717Z","iopub.status.idle":"2022-04-03T02:19:42.146301Z","shell.execute_reply.started":"2022-04-03T02:19:42.076676Z","shell.execute_reply":"2022-04-03T02:19:42.145422Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ntrn_data.head(10)","metadata":{"execution":{"iopub.status.busy":"2022-04-03T02:19:42.147807Z","iopub.execute_input":"2022-04-03T02:19:42.148078Z","iopub.status.idle":"2022-04-03T02:19:42.169935Z","shell.execute_reply.started":"2022-04-03T02:19:42.14804Z","shell.execute_reply":"2022-04-03T02:19:42.169228Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ntrn_data.describe()","metadata":{"execution":{"iopub.status.busy":"2022-04-03T02:19:42.171873Z","iopub.execute_input":"2022-04-03T02:19:42.172281Z","iopub.status.idle":"2022-04-03T02:19:42.95865Z","shell.execute_reply.started":"2022-04-03T02:19:42.172244Z","shell.execute_reply":"2022-04-03T02:19:42.95794Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ntrn_data.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2022-04-03T02:19:42.960291Z","iopub.execute_input":"2022-04-03T02:19:42.960812Z","iopub.status.idle":"2022-04-03T02:19:43.025362Z","shell.execute_reply.started":"2022-04-03T02:19:42.960776Z","shell.execute_reply":"2022-04-03T02:19:43.024428Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"## 4.2. Analysing the Trian Labels Dataset","metadata":{}},{"cell_type":"code","source":"%%time\ntrn_label_data.info()","metadata":{"execution":{"iopub.status.busy":"2022-04-03T02:19:43.026982Z","iopub.execute_input":"2022-04-03T02:19:43.027271Z","iopub.status.idle":"2022-04-03T02:19:43.039515Z","shell.execute_reply.started":"2022-04-03T02:19:43.027232Z","shell.execute_reply":"2022-04-03T02:19:43.038484Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ntrn_label_data.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-03T02:19:43.041385Z","iopub.execute_input":"2022-04-03T02:19:43.042056Z","iopub.status.idle":"2022-04-03T02:19:43.055916Z","shell.execute_reply.started":"2022-04-03T02:19:43.042013Z","shell.execute_reply":"2022-04-03T02:19:43.055202Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ntrn_label_data.describe()","metadata":{"execution":{"iopub.status.busy":"2022-04-03T02:19:43.05733Z","iopub.execute_input":"2022-04-03T02:19:43.057822Z","iopub.status.idle":"2022-04-03T02:19:43.077922Z","shell.execute_reply.started":"2022-04-03T02:19:43.057783Z","shell.execute_reply":"2022-04-03T02:19:43.077037Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ntrn_label_data.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2022-04-03T02:19:43.07937Z","iopub.execute_input":"2022-04-03T02:19:43.079855Z","iopub.status.idle":"2022-04-03T02:19:43.091472Z","shell.execute_reply.started":"2022-04-03T02:19:43.079809Z","shell.execute_reply":"2022-04-03T02:19:43.090245Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"## 4.3. Analysing the Trian Dataset, Using Groups","metadata":{}},{"cell_type":"code","source":"%%time\ntrn_summary = trn_data[['sequence', 'subject', 'step']].groupby(['sequence', 'subject']).count().reset_index()","metadata":{"execution":{"iopub.status.busy":"2022-04-03T02:19:43.095472Z","iopub.execute_input":"2022-04-03T02:19:43.095757Z","iopub.status.idle":"2022-04-03T02:19:43.215477Z","shell.execute_reply.started":"2022-04-03T02:19:43.095718Z","shell.execute_reply":"2022-04-03T02:19:43.214706Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ntrn_summary[trn_summary['subject'] == 66].shape","metadata":{"execution":{"iopub.status.busy":"2022-04-03T02:19:43.21687Z","iopub.execute_input":"2022-04-03T02:19:43.21746Z","iopub.status.idle":"2022-04-03T02:19:43.227323Z","shell.execute_reply.started":"2022-04-03T02:19:43.21742Z","shell.execute_reply":"2022-04-03T02:19:43.226426Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nsummary_by_subject = trn_summary[['sequence', 'subject']].groupby(['subject']).count().reset_index()\nsummary_by_subject.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-03T02:19:43.228678Z","iopub.execute_input":"2022-04-03T02:19:43.229572Z","iopub.status.idle":"2022-04-03T02:19:43.243882Z","shell.execute_reply.started":"2022-04-03T02:19:43.229536Z","shell.execute_reply":"2022-04-03T02:19:43.243113Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ntrn_unique_subjects = set(list(trn_data['subject'].unique()))\ntst_unique_subjects = set(list(tst_data['subject'].unique()))\noverlap_subjets = trn_unique_subjects.intersection(tst_unique_subjects)\nprint('Repeated Subjects in Test Dataset:', len(overlap_subjets))","metadata":{"execution":{"iopub.status.busy":"2022-04-03T02:19:43.245121Z","iopub.execute_input":"2022-04-03T02:19:43.24541Z","iopub.status.idle":"2022-04-03T02:19:43.266462Z","shell.execute_reply.started":"2022-04-03T02:19:43.245376Z","shell.execute_reply":"2022-04-03T02:19:43.265829Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"# 5. Exploratory Data Analysis","metadata":{}},{"cell_type":"markdown","source":"## 5.1. Visualization and Others (Under Construction!)","metadata":{}},{"cell_type":"markdown","source":"# 6. Creating New Model Features","metadata":{}},{"cell_type":"markdown","source":"## 6.1. Creating Aggregated Features by Subject and Sequence","metadata":{}},{"cell_type":"code","source":"%%time\nfrom scipy.stats import kurtosis\ndef kurtosis_func(series):\n    '''\n    Describe something...\n    '''\n    return kurtosis(series)\n\ndef q01(series):\n    return np.quantile(series, 0.01)\n\ndef q05(series):\n    return np.quantile(series, 0.05)\n\ndef q95(series):\n    return np.quantile(series, 0.95)\n\ndef q99(series):\n    return np.quantile(series, 0.99)\n\ndef aggregated_features(df, aggregation_cols = ['sequence'], prefix = ''):\n    agg_strategy = {'sensor_00': ['mean', 'max', 'min', 'var', 'mad', 'sum', 'median', 'skew', kurtosis_func, q01, q05, q95, q99],\n                    'sensor_01': ['mean', 'max', 'min', 'var', 'mad', 'sum', 'median', 'skew', kurtosis_func, q01, q05, q95, q99],\n                    'sensor_02': ['mean', 'max', 'min', 'var', 'mad', 'sum', 'median', 'skew', kurtosis_func, q01, q05, q95, q99],\n                    'sensor_03': ['mean', 'max', 'min', 'var', 'mad', 'sum', 'median', 'skew', kurtosis_func, q01, q05, q95, q99],\n                    'sensor_04': ['mean', 'max', 'min', 'var', 'mad', 'sum', 'median', 'skew', kurtosis_func, q01, q05, q95, q99],\n                    'sensor_05': ['mean', 'max', 'min', 'var', 'mad', 'sum', 'median', 'skew', kurtosis_func, q01, q05, q95, q99],\n                    'sensor_06': ['mean', 'max', 'min', 'var', 'mad', 'sum', 'median', 'skew', kurtosis_func, q01, q05, q95, q99],\n                    'sensor_07': ['mean', 'max', 'min', 'var', 'mad', 'sum', 'median', 'skew', kurtosis_func, q01, q05, q95, q99],\n                    'sensor_08': ['mean', 'max', 'min', 'var', 'mad', 'sum', 'median', 'skew', kurtosis_func, q01, q05, q95, q99],\n                    'sensor_09': ['mean', 'max', 'min', 'var', 'mad', 'sum', 'median', 'skew', kurtosis_func, q01, q05, q95, q99],\n                    'sensor_10': ['mean', 'max', 'min', 'var', 'mad', 'sum', 'median', 'skew', kurtosis_func, q01, q05, q95, q99],\n                    'sensor_11': ['mean', 'max', 'min', 'var', 'mad', 'sum', 'median', 'skew', kurtosis_func, q01, q05, q95, q99],\n                    'sensor_12': ['mean', 'max', 'min', 'var', 'mad', 'sum', 'median', 'skew', kurtosis_func, q01, q05, q95, q99],\n                   }\n    group = df.groupby(aggregation_cols).aggregate(agg_strategy)\n    group.columns = ['_'.join(col).strip() for col in group.columns]\n    group.columns = [str(prefix) + str(col) for col in group.columns]\n    group.reset_index(inplace = True)\n    \n    temp = (df.groupby(aggregation_cols).size().reset_index(name = str(prefix) + 'size'))\n    group = pd.merge(temp, group, how = 'left', on = aggregation_cols,)\n    return group","metadata":{"execution":{"iopub.status.busy":"2022-04-03T02:19:43.743137Z","iopub.execute_input":"2022-04-03T02:19:43.743758Z","iopub.status.idle":"2022-04-03T02:19:44.117849Z","shell.execute_reply.started":"2022-04-03T02:19:43.74372Z","shell.execute_reply":"2022-04-03T02:19:44.117128Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ntrn_merge_data = aggregated_features(trn_data, aggregation_cols = ['sequence', 'subject'])\ntst_merge_data = aggregated_features(tst_data, aggregation_cols = ['sequence', 'subject'])","metadata":{"execution":{"iopub.status.busy":"2022-04-03T02:19:44.119308Z","iopub.execute_input":"2022-04-03T02:19:44.119559Z","iopub.status.idle":"2022-04-03T02:28:05.031344Z","shell.execute_reply.started":"2022-04-03T02:19:44.119524Z","shell.execute_reply":"2022-04-03T02:28:05.030555Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 6.2. Creating Aggregated Features by Subject","metadata":{}},{"cell_type":"code","source":"%%time\ntrn_subjects_merge_data = aggregated_features(trn_data, aggregation_cols = ['subject'], prefix = 'subject_')\ntst_subjects_merge_data = aggregated_features(tst_data, aggregation_cols = ['subject'], prefix = 'subject_')","metadata":{"execution":{"iopub.status.busy":"2022-04-03T02:28:05.032667Z","iopub.execute_input":"2022-04-03T02:28:05.033096Z","iopub.status.idle":"2022-04-03T02:28:26.901387Z","shell.execute_reply.started":"2022-04-03T02:28:05.033057Z","shell.execute_reply":"2022-04-03T02:28:26.900582Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ntrn_subjects_merge_data.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-03T02:28:26.902768Z","iopub.execute_input":"2022-04-03T02:28:26.90322Z","iopub.status.idle":"2022-04-03T02:28:26.921215Z","shell.execute_reply.started":"2022-04-03T02:28:26.903167Z","shell.execute_reply":"2022-04-03T02:28:26.92053Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"## 6.3 Experimenting with Lags (Under Construction!)","metadata":{}},{"cell_type":"code","source":"%%time\ntrn_data['sensor_00_lag_01'] = trn_data['sensor_00'].shift(1)\ntrn_data['sensor_00_lag_10'] = trn_data['sensor_00'].shift(10)","metadata":{"execution":{"iopub.status.busy":"2022-04-03T02:28:26.922285Z","iopub.execute_input":"2022-04-03T02:28:26.922925Z","iopub.status.idle":"2022-04-03T02:28:26.93931Z","shell.execute_reply.started":"2022-04-03T02:28:26.922886Z","shell.execute_reply":"2022-04-03T02:28:26.938614Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 7. Merging the Datasets for Training","metadata":{}},{"cell_type":"code","source":"%%time\ntrn_merge_data = trn_merge_data.merge(trn_label_data, how = 'left', on = 'sequence')","metadata":{"execution":{"iopub.status.busy":"2022-04-03T02:28:26.97405Z","iopub.execute_input":"2022-04-03T02:28:26.974367Z","iopub.status.idle":"2022-04-03T02:28:27.002977Z","shell.execute_reply.started":"2022-04-03T02:28:26.974329Z","shell.execute_reply":"2022-04-03T02:28:27.001869Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ntrn_merge_data = trn_merge_data.merge(trn_subjects_merge_data, how = 'left', on = 'subject')\ntst_merge_data = tst_merge_data.merge(tst_subjects_merge_data, how = 'left', on = 'subject')","metadata":{"execution":{"iopub.status.busy":"2022-04-03T02:28:27.004508Z","iopub.execute_input":"2022-04-03T02:28:27.00479Z","iopub.status.idle":"2022-04-03T02:28:27.055914Z","shell.execute_reply.started":"2022-04-03T02:28:27.004752Z","shell.execute_reply":"2022-04-03T02:28:27.055107Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ntrn_merge_data.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-03T02:28:27.060073Z","iopub.execute_input":"2022-04-03T02:28:27.060599Z","iopub.status.idle":"2022-04-03T02:28:27.075946Z","shell.execute_reply.started":"2022-04-03T02:28:27.060567Z","shell.execute_reply":"2022-04-03T02:28:27.075245Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ntst_merge_data.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-03T02:28:27.077009Z","iopub.execute_input":"2022-04-03T02:28:27.077639Z","iopub.status.idle":"2022-04-03T02:28:27.095307Z","shell.execute_reply.started":"2022-04-03T02:28:27.077602Z","shell.execute_reply":"2022-04-03T02:28:27.09443Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"# 8. Post Processing the Information for the Model","metadata":{}},{"cell_type":"code","source":"%%time\nignore = ['sequence', 'state', 'subject']\nfeatures = [feat for feat in trn_merge_data.columns if feat not in ignore]\ntarget_feature = 'state'","metadata":{"execution":{"iopub.status.busy":"2022-04-03T02:28:27.096457Z","iopub.execute_input":"2022-04-03T02:28:27.097133Z","iopub.status.idle":"2022-04-03T02:28:27.105706Z","shell.execute_reply.started":"2022-04-03T02:28:27.097096Z","shell.execute_reply":"2022-04-03T02:28:27.104965Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"# 9. Creating a Simple Train / Test Split Strategy","metadata":{}},{"cell_type":"code","source":"%%time\nfrom sklearn.model_selection import train_test_split\ntest_size_pct = 0.10\nX_train, X_valid, y_train, y_valid = train_test_split(trn_merge_data[features], trn_merge_data[target_feature], test_size = test_size_pct, random_state = 42)","metadata":{"execution":{"iopub.status.busy":"2022-04-03T02:28:27.106873Z","iopub.execute_input":"2022-04-03T02:28:27.10757Z","iopub.status.idle":"2022-04-03T02:28:27.279511Z","shell.execute_reply.started":"2022-04-03T02:28:27.107533Z","shell.execute_reply":"2022-04-03T02:28:27.278748Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"# 10. Building a Baseline GBT Model","metadata":{}},{"cell_type":"code","source":"%%time\nfrom xgboost  import XGBClassifier","metadata":{"execution":{"iopub.status.busy":"2022-04-03T02:28:27.280766Z","iopub.execute_input":"2022-04-03T02:28:27.281308Z","iopub.status.idle":"2022-04-03T02:28:27.305284Z","shell.execute_reply.started":"2022-04-03T02:28:27.281265Z","shell.execute_reply":"2022-04-03T02:28:27.304599Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"params = {'n_estimators': 4096,\n          'max_depth': 7,\n          'learning_rate': 0.15,\n          'subsample': 0.95,\n          'colsample_bytree': 0.60,\n          'reg_lambda': 1.50,\n          'reg_alpha': 6.10,\n          'gamma': 1.40,\n          'random_state': 69,\n          'objective': 'binary:logistic',\n          'tree_method': 'gpu_hist',\n         }","metadata":{"execution":{"iopub.status.busy":"2022-04-03T02:28:27.306868Z","iopub.execute_input":"2022-04-03T02:28:27.307127Z","iopub.status.idle":"2022-04-03T02:28:27.311764Z","shell.execute_reply.started":"2022-04-03T02:28:27.307092Z","shell.execute_reply":"2022-04-03T02:28:27.310965Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nxgb = XGBClassifier(**params)\nxgb.fit(X_train, y_train, eval_set = [(X_valid, y_valid)], eval_metric = ['auc'], early_stopping_rounds = 128, verbose = 50)","metadata":{"execution":{"iopub.status.busy":"2022-04-03T02:28:27.313251Z","iopub.execute_input":"2022-04-03T02:28:27.313712Z","iopub.status.idle":"2022-04-03T02:28:31.985906Z","shell.execute_reply.started":"2022-04-03T02:28:27.313675Z","shell.execute_reply":"2022-04-03T02:28:31.985075Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nfrom sklearn.metrics import roc_auc_score\npreds = xgb.predict_proba(X_valid)[:, 1]\nscore = roc_auc_score(y_valid, preds)\nprint(score)","metadata":{"execution":{"iopub.status.busy":"2022-04-03T02:28:31.98721Z","iopub.execute_input":"2022-04-03T02:28:31.987541Z","iopub.status.idle":"2022-04-03T02:28:32.036856Z","shell.execute_reply.started":"2022-04-03T02:28:31.987501Z","shell.execute_reply":"2022-04-03T02:28:32.03634Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"# 11. Undertanding Model Behavior, Feature Importance","metadata":{}},{"cell_type":"code","source":"%%time\ndef plot_feature_importance(importance, names, model_type, max_features = 10):\n    #Create arrays from feature importance and feature names\n    feature_importance = np.array(importance)\n    feature_names = np.array(names)\n\n    #Create a DataFrame using a Dictionary\n    data={'feature_names':feature_names,'feature_importance':feature_importance}\n    fi_df = pd.DataFrame(data)\n\n    #Sort the DataFrame in order decreasing feature importance\n    fi_df.sort_values(by=['feature_importance'], ascending=False,inplace=True)\n    fi_df = fi_df.head(max_features)\n\n    #Define size of bar plot\n    plt.figure(figsize=(8,6))\n    \n    #Plot Searborn bar chart\n    sns.barplot(x=fi_df['feature_importance'], y=fi_df['feature_names'])\n    #Add chart labels\n    plt.title(model_type + 'FEATURE IMPORTANCE')\n    plt.xlabel('FEATURE IMPORTANCE')\n    plt.ylabel('FEATURE NAMES')","metadata":{"execution":{"iopub.status.busy":"2022-04-03T02:28:32.039961Z","iopub.execute_input":"2022-04-03T02:28:32.041465Z","iopub.status.idle":"2022-04-03T02:28:32.050641Z","shell.execute_reply.started":"2022-04-03T02:28:32.041432Z","shell.execute_reply":"2022-04-03T02:28:32.049943Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nplot_feature_importance(xgb.feature_importances_,X_train.columns,'XG BOOST ', max_features = 20)","metadata":{"execution":{"iopub.status.busy":"2022-04-03T02:29:48.184214Z","iopub.execute_input":"2022-04-03T02:29:48.18478Z","iopub.status.idle":"2022-04-03T02:29:48.535679Z","shell.execute_reply.started":"2022-04-03T02:29:48.184742Z","shell.execute_reply":"2022-04-03T02:29:48.534881Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"# 12. Baseline Model Submission File Generation","metadata":{}},{"cell_type":"code","source":"%%time\nfrom sklearn.metrics import roc_auc_score\npreds = xgb.predict_proba(tst_merge_data[features])[:, 1]","metadata":{"execution":{"iopub.status.busy":"2022-04-03T02:28:32.384401Z","iopub.execute_input":"2022-04-03T02:28:32.38465Z","iopub.status.idle":"2022-04-03T02:28:32.582086Z","shell.execute_reply.started":"2022-04-03T02:28:32.384615Z","shell.execute_reply":"2022-04-03T02:28:32.581254Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nsub['state'] = preds\nsub.to_csv('my_submission_041222.csv', index = False)","metadata":{"execution":{"iopub.status.busy":"2022-04-03T02:28:32.583499Z","iopub.execute_input":"2022-04-03T02:28:32.584281Z","iopub.status.idle":"2022-04-03T02:28:32.623446Z","shell.execute_reply.started":"2022-04-03T02:28:32.584239Z","shell.execute_reply":"2022-04-03T02:28:32.622319Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---","metadata":{}}]}